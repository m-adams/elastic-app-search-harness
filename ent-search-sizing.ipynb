{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script is designed to do somee simple tests for Enterprise Search using Elastic Cloud\n",
    "# It will use the Elastic Cloud CLI. Please asetup as per the docs\n",
    "# https://www.elastic.co/guide/en/ecctl/current/index.html\n",
    "# on Mac use\n",
    "# brew tap elastic/tap\n",
    "# brew install elastic/tap/ecctl\n",
    "# ecctl init \n",
    "# Need to add the App Search specific python client\n",
    "# python -m pip install elastic-app-search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "engine_name = \"sizing\"\n",
    "\n",
    "doc_size = 200\n",
    "batch_size = 100\n",
    "number_of_batches = 1000\n",
    "\n",
    "cluster_info_path= \"./cluster-info.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import subprocess\n",
    "from requests.auth import HTTPBasicAuth\n",
    "from urllib.parse import urljoin\n",
    "from elastic_app_search import Client\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "from nltk import download as nltk_download\n",
    "from nltk.corpus import words\n",
    "from random import sample\n",
    "nltk_download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets setup the env and do a smoke test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So there is currently a bug in ecctl which should be fixed soon.\n",
    "# until then please run the command directly in the terminal to create the file.\n",
    "# when the issue is fixed we can reuse this code\n",
    "#\n",
    "# ecctl deployment create -f ./deployment-config.json > cluster-info.json\n",
    "\n",
    "#print(\"Checking if the cluster is already created\")\n",
    "#if not(os.path.exists(cluster_info_path) and os.path.getsize(cluster_info_path)>0):\n",
    "#    print(\"No cluster info file so lets create a new cluster in cloud\")\n",
    "#    info_file=open(cluster_info_path,\"w+\")\n",
    "#    out = subprocess.run(args=['ecctl', 'deployment', 'create', '-f ./deployment-config.json'], \n",
    "#           stdout=info_file, \n",
    "#           stderr=subprocess.STDOUT)\n",
    "#    stdout,stderr = out.communicate()\n",
    "#    print(stdout,stderr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Reading cluster info file: \",cluster_info_path)\n",
    "with open(cluster_info_path) as json_file:\n",
    "    cluster_info = json.load(json_file)\n",
    "    deployment_name = cluster_info['name']\n",
    "    #print(\"Cluster name: \",deployment_name)\n",
    "    deployment_id = cluster_info['id']\n",
    "    #print(\"Cluster ID: \",deployment_id)\n",
    "    cloud_id = cluster_info['resources'][0]['cloud_id']\n",
    "    #print(\"CloudID: \",cloud_id)\n",
    "    elastic_password = cluster_info['resources'][0]['credentials']['password']\n",
    "    #print(\"elastiic password: \",elastic_password)\n",
    "    cluster_exists = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the endpoints for App Search, Elasticsearch etc we need to run a second command\n",
    "# This will get all the details of the cluster and return json\n",
    "out = subprocess.run(['ecctl', 'deployment', 'show', deployment_id], \n",
    "           stdout=subprocess.PIPE, \n",
    "           stderr=subprocess.STDOUT)\n",
    "cluster_details = json.loads(out.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can extract any details we need\n",
    "as_endpoint = cluster_details['resources']['enterprise_search'][0]['info']['metadata']['endpoint']\n",
    "#print('App Search endpoint: ',as_endpoint)\n",
    "es_endpoint = cluster_details['resources']['elasticsearch'][0]['info']['metadata']['endpoint']\n",
    "#print('Elasticsearch endpoint: ',es_endpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The App Search Client needs an API key so we will need to call the API directly using basic auth to generate a key\n",
    "\n",
    "# The only way to get this atm is through an undocumented API\n",
    "create_key_url = urljoin(\"https://\"+as_endpoint, '/as/credentials/collection?page[current]=1')\n",
    "#print(create_key_url)\n",
    "response =requests.get(create_key_url, auth=('elastic', elastic_password))\n",
    "#print(response)\n",
    "api_key_info = response.json()\n",
    "for key in api_key_info['results']:\n",
    "    if key['type'] == \"private\":\n",
    "        private_key = key['key']\n",
    "        #print(\"Private key: \",private_key)\n",
    "    elif key['type'] == \"search\":\n",
    "        search_key = key['key']\n",
    "        #print(\"Search key: \",search_key)\n",
    "#as_client = Client("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The App Search client need s base endpoint which included the API path \n",
    "base_endpoint = as_endpoint+'/api/as/v1'\n",
    "as_client = Client(\n",
    "    base_endpoint=base_endpoint,\n",
    "    api_key=private_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_client.create_engine(engine_name, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do a simple ingest and query before we delete the engine as a smoke test\n",
    "\n",
    "documents = [\n",
    "    {\n",
    "      'id': 'INscMGmhmX4',\n",
    "      'url': 'https://www.youtube.com/watch?v=INscMGmhmX4',\n",
    "      'title': 'The Original Grumpy Cat',\n",
    "      'body': 'A wonderful video of a magnificent cat.'\n",
    "    },\n",
    "    {\n",
    "      'id': 'JNDFojsd02',\n",
    "      'url': 'https://www.youtube.com/watch?v=dQw4w9WgXcQ',\n",
    "      'title': 'Another Grumpy Cat',\n",
    "      'body': 'A great video of another cool cat.'\n",
    "    }\n",
    "]\n",
    "\n",
    "as_client.index_documents(engine_name, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets search for something\n",
    "as_client.search(engine_name, 'grumpy cat', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's destroy the engine\n",
    "as_client.destroy_engine(engine_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO for our experiment, let's create a document generator to be able to load the engine\n",
    "\n",
    "def generate_doc():\n",
    "    \"\"\"Generates a sinle example text document for indexing\"\"\"\n",
    "    # The current document will be simple\n",
    "    # 3 word title\n",
    "    # 2 word category\n",
    "    # body length determined by the parameter at the top of the file\n",
    "    title = ' '.join(sample(words.words(), 3))\n",
    "    category = ' '.join(sample(words.words(), 2))\n",
    "    body = ' '.join(sample(words.words(), doc_size))\n",
    "    doc={\"title\":title,\"category\":category,\"body\":body}\n",
    "    # Lets calculate a rough number of bytes for the text\n",
    "    # we should just be able to use _size in elasticsearch but this is abackup and will be very close\n",
    "    str_size=len(json.dumps(doc).encode('utf-8'))\n",
    "    doc['str_size']=str_size\n",
    "    return doc\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create an engine to index in to\n",
    "as_client.create_engine(engine_name, 'en')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also initialise the connection to Elasticsearch directly\n",
    "client = Elasticsearch()\n",
    "es_client = Elasticsearch(\n",
    "    cloud_id=cloud_id,\n",
    "    http_auth=(\"elastic\", elastic_password),\n",
    ")\n",
    "\n",
    "# We need to create the index with the mapper size plugin\n",
    "# and it turned on to map the size of the docs\n",
    "\n",
    "# We can modify this mapping later to see the affect of different analysers, mappings etc\n",
    "index_body=\"\"\"{\n",
    "  \"mappings\": {\n",
    "    \"_size\": {\n",
    "      \"enabled\": true\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "\n",
    "es_client.indices.create(engine_name,body=index_body)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def index_batch(batch_size):\n",
    "    \"\"\"\n",
    "    Generates a batch of documents \n",
    "    and then indexes them in to App Search and Elasticsearch directly\n",
    "    \"\"\"\n",
    "    # Generate docs\n",
    "    documents=[]\n",
    "    for i in range(batch_size):\n",
    "        doc=generate_doc()\n",
    "        documents.append(doc)\n",
    "    \n",
    "    # Index in to AS and then check the responses\n",
    "    results = as_client.index_documents(engine_name, documents)\n",
    "    for doc_result in results:\n",
    "        if len(doc_result['errors'])>0:\n",
    "            raise Exception(\"Got errors back from App search: \"+\" \".join(doc_result['errors']))\n",
    "            \n",
    "    # Index in to Elasticsearch\n",
    "    # Lest's build a _bulk body\n",
    "    opperation_json= json.dumps({ \"index\":{} })+\"\\n\"\n",
    "    bulk_json = \"\"\n",
    "    for doc in documents:\n",
    "        bulk_json = bulk_json+opperation_json\n",
    "        bulk_json = bulk_json+ json.dumps(doc)+\"\\n\"\n",
    "    bulk_response = es_client.bulk(body=bulk_json,index=engine_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_es_stats():\n",
    "    \"\"\"Gather stats on the size of the source and the indexes so that we can track\"\"\"\n",
    "    \n",
    "    stats={}\n",
    "    \n",
    "    # We can run an aggregation to sum the _size field accross the docs \n",
    "    # This should give us the aprox size of the source data\n",
    "    size_agg_body = \"\"\"{\n",
    "  \"size\": 0, \n",
    "  \"aggs\": {\n",
    "    \"total_size\": {\n",
    "      \"sum\": {\n",
    "        \"field\": \"_size\"\n",
    "      }\n",
    "    },\n",
    "    \"total_str_size\":{\n",
    "      \"sum\": {\n",
    "        \"field\": \"str_size\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\"\"\"\n",
    "    # We can aggregate the size of the doc direct as measured at ingest\n",
    "    agg_response=es_client.search(index=engine_name, body=size_agg_body)\n",
    "    stats['total_size'] = agg_response['aggregations']['total_size']['value']\n",
    "    stats['total_str_size'] = agg_response['aggregations']['total_str_size']['value']\n",
    "    \n",
    "    # We can loop through the indexes and record their size but we probably also want a total for Ent Search\n",
    "    # It would be good in the future to also split Ent Search in to logs/analytics and everything else\n",
    "    for index_name in [engine_name,'.ent*']:\n",
    "        cat_response = es_client.cat.indices(index=index_name,format=\"json\",bytes='b')\n",
    "        if index_name == '.ent*':\n",
    "            stats['total_ent_search_primary_size']=0\n",
    "            stats['total_ent_search_store_size']=0\n",
    "            stats['total_ent_search_doc_count']=0\n",
    "        for index in cat_response:\n",
    "            name= index['index'].strip('.')\n",
    "            stats[name+'_primary_size']=int(index['pri.store.size'])\n",
    "            stats[name+'_store_size']=int(index['store.size'])\n",
    "            stats[name+'_docs_count']=int(index['docs.count'])\n",
    "            if index_name == '.ent*':\n",
    "                stats['total_ent_search_primary_size']+=int(index['pri.store.size'])\n",
    "                stats['total_ent_search_store_size']+=int(index['store.size'])\n",
    "                stats['total_ent_search_doc_count']+=int(index['docs.count'])\n",
    "        \n",
    "    return stats\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's put it all together\n",
    "# Run one batch as a time, index in to both \n",
    "# Get stats after each batch and then index those stats back in to Elasticsearch\n",
    "for batch in range(number_of_batches):\n",
    "    index_batch(batch_size)\n",
    "    es_client.indices.refresh(index=\"*\")\n",
    "    es_client.indices.flush(index=\"*\")\n",
    "    stats = get_es_stats()\n",
    "    #print(stats)\n",
    "    # Lets index the stats in to Elasticsearch as a new index\n",
    "    es_client.index(index='stats',body=json.dumps(stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to reset things let's delete what we have created\n",
    "# Now let's destroy the engine\n",
    "#try: \n",
    "#    as_client.destroy_engine(engine_name)\n",
    "#except:\n",
    "#    pass\n",
    "        \n",
    "#es_client.indices.delete(index=engine_name)\n",
    "#es_client.indices.delete(index='stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
